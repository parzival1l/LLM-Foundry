{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Schema Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import instructor\n",
    "from typing import Optional, Any, get_type_hints\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml_schema(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        schema = yaml.safe_load(file)\n",
    "    return schema\n",
    "\n",
    "filepath = '/Users/nandy/Documents/GitHub/LLM-Foundry/configs/ouput_schema.yaml'\n",
    "schema = load_yaml_schema(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    \"str\": str,\n",
    "    \"int\": int,\n",
    "    \"float\": float,\n",
    "    \"bool\": bool,\n",
    "    \"Optional[str]\": Optional[str],\n",
    "    \"Optional[int]\": Optional[int],\n",
    "    \"Optional[float]\": Optional[float],\n",
    "    \"Optional[bool]\": Optional[bool],\n",
    "    \"Any\": Any,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'ingredient': {'description': 'Name of the ingredient', 'title': 'Ingredient', 'type': 'string'}, 'quantity': {'description': 'Quantity of the ingredient', 'title': 'Quantity', 'type': 'number'}, 'process': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'description': 'Process of the ingredient eg., chopped, sliced, etc.', 'title': 'Process'}}, 'required': ['ingredient', 'quantity', 'process'], 'title': 'FoodSchema', 'type': 'object'}\n",
      "ingredient='Tomato' quantity=2.5 process='chopped'\n"
     ]
    }
   ],
   "source": [
    "def create_pydantic_model(class_name: str, schema: dict, type_mapping: dict):\n",
    "    fields = {}\n",
    "    for field_name, field_info in schema.items():\n",
    "        field_type_str = field_info.get('type')\n",
    "        field_description = field_info.get('description', '')\n",
    "\n",
    "        # Resolve the type using the type_mapping\n",
    "        field_type = type_mapping.get(field_type_str, Any)\n",
    "\n",
    "        # Add the field to the fields dictionary with a FieldInfo\n",
    "        fields[field_name] = (field_type, Field(description=field_description))\n",
    "\n",
    "    # Dynamically create the Pydantic model using create_model\n",
    "    return create_model(class_name, **fields)\n",
    "\n",
    "# Load schema from YAML\n",
    "schema_yaml = load_yaml_schema(file_path=filepath)\n",
    "\n",
    "# Create and use the Pydantic model\n",
    "for class_name, schema in schema_yaml.items():\n",
    "    model = create_pydantic_model(class_name, schema, type_mapping)\n",
    "    print(model.schema())\n",
    "\n",
    "    # Example usage of the model\n",
    "    # example_data = {\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}\n",
    "    example_data = {\n",
    "        \"ingredient\": \"Tomato\",\n",
    "        \"quantity\": 2.5,\n",
    "        \"process\": \"chopped\"\n",
    "        }\n",
    "    instance = model(**example_data)\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredient='Tomato' quantity=2.5 process='chopped'\n"
     ]
    }
   ],
   "source": [
    "instance = model(**example_data)\n",
    "print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.S Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_4o = \"gpt-4o-2024-08-06\"\n",
    "GPT_4o_mini = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "def return_oai_response(inputs) :\n",
    "\n",
    "  GPT_4o = \"gpt-4o-2024-08-06\"\n",
    "  GPT_4o_mini = \"gpt-4o-mini\"\n",
    "\n",
    "  user_info = client.chat.completions.create(\n",
    "      model=GPT_4o_mini,\n",
    "      temperature=0.05,\n",
    "        max_tokens=1250,\n",
    "        top_p=0.05,\n",
    "        frequency_penalty=0.1,\n",
    "        presence_penalty=1,\n",
    "      response_model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": inputs['content']}],\n",
    "  )\n",
    "  return user_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langsmith Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "client = wrap_openai(openai.Client())\n",
    "client = instructor.from_openai(client, mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langsmith.schemas import Run, Example\n",
    "# Evaluators\n",
    "\n",
    "dataset_name = \"Sample-NER-test\"\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    student_answer = run.outputs.get(\"output\")\n",
    "    # example.outputs\n",
    "    # Check if the student_answer is an empty string\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "qa_evalulator = [is_answered]\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    return_oai_response,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-instructor-fc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-custom-fc-test-ce85c72b' at:\n",
      "https://smith.langchain.com/o/8c28e029-234e-55f2-addf-105eb152accb/datasets/85e68a36-80f0-47eb-800e-f21db38a1792/compare?selectedSessions=b9daa867-3b91-49c5-9606-5ddbe945ed41\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c979c75b9e64988807be875bbc67eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "from langsmith.schemas import Run, Example\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similarity_score(a, b):\n",
    "    \"\"\"Calculates similarity score between two strings.\"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def evaluate_dict(run: Run, example: Example) -> dict:\n",
    "    output = run.outputs.get(\"output\").dict()\n",
    "    reference = example.outputs\n",
    "    score = 0\n",
    "    keys = [\n",
    "        ('ingredient', 'ingredient'),\n",
    "        ('quantity', 'Quantity'),\n",
    "        ('process', 'Process')\n",
    "    ]\n",
    "\n",
    "    for ref_key, out_key in keys:\n",
    "        ref_value = reference.get(ref_key, None)\n",
    "        out_value = output.get(out_key, None)\n",
    "\n",
    "        # Normalize None and empty strings\n",
    "        if ref_value is None and out_value in [None, '']:\n",
    "            score += 1\n",
    "            continue\n",
    "\n",
    "        # Convert strings to lowercase for comparison\n",
    "        if isinstance(ref_value, str) and isinstance(out_value, str):\n",
    "            ref_value = ref_value.lower()\n",
    "            out_value = out_value.lower()\n",
    "\n",
    "        # Compare the values and update the score\n",
    "        if ref_value == out_value:\n",
    "            score += 1\n",
    "        else:\n",
    "            # Apply partial scoring for non-exact matches\n",
    "            if isinstance(ref_value, str) and isinstance(out_value, str):\n",
    "                score += similarity_score(ref_value, out_value)\n",
    "            else:\n",
    "                score += 0  # No partial score for non-string mismatches\n",
    "\n",
    "    # Normalize the score to be out of 1 (or 100 if you prefer percentages)\n",
    "    max_score = len(keys)  # total possible score for exact match\n",
    "    normalized_score = score / max_score\n",
    "    return {\"key\": \"EvaluationResults\", \"score\": normalized_score*100}\n",
    "\n",
    "dataset_name = \"Sample-NER-test\"\n",
    "qa_evalulator = [evaluate_dict]\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    return_oai_response,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-custom-fc-test\",\n",
    ")\n",
    "\n",
    "# # Example usage\n",
    "# A = {'ingredient': 'refined sugar', 'quantity': 21.5, 'process': None}\n",
    "# B = {'Process': '', 'Quantity': 21.5, 'ingredient': 'refined sugar'}\n",
    "\n",
    "# evaluation = evaluate_dict(A, B)\n",
    "# print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt based Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_output = [{\n",
    "    \"ing\": \"name of ingredient.\",\n",
    "    \"qty\": \"the quantity is a number or fraction only formatted as a string e.g. 1/2, 1, 1 1/2, 1.5, 0.5, 1-1/2\",\n",
    "    \"uom\": \"options: '' or the unit of measure\",\n",
    "    \"is_food_item\": \"true / false if this is an actual food item\",\n",
    "    \"brand\": \"options : '' or commercial brand of input if present\",\n",
    "    \"weight\": \"weight is a number or fraction only, formatted as a string\",\n",
    "    \"weight_uom\": \"weight unit of measure\",\n",
    "    \"volume\": \"volume is a number or fraction only, formatted as a string\",\n",
    "    \"volume_uom\": \"volume unit of measure\",\n",
    "    \"length\": \"length is a number or fraction only, formatted as a string\",\n",
    "    \"length_uom\": \"length unit of measure\",\n",
    "    \"width\": \"width is a number or fraction only, formatted as a string\",\n",
    "    \"width_uom\": \"width unit of measure\",\n",
    "    \"height\": \"height is a number or fraction only, formatted as a string\",\n",
    "    \"height_uom\": \"height unit of measure\",\n",
    "    \"part\": \"options : '' or the part of a meat ingredient e.g. beef chin -> chin, chicken breast => breast\",\n",
    "    \"process\": \"transformation done to the ingredient e.g. sliced, shredded, cut into something\",\n",
    "    \"pq\": \"physical quality of the product : e.g. 20% sodium, 2% fat, lean #/fat # %\",\n",
    "    \"purpose\": \"options: '' or purpose e.g. for frying, for serving\",\n",
    "    #\"origin\": \"when the 'ing' is extracted from the origin e.g. juice from lemon => origin = lemon\",\n",
    "    \"multiple_keyword\": \"options: '' for single ing | 'or' for alternative options | 'and' for multiple required ingredients\",\n",
    "    \"culture\": \"two letter iso code of the language of the ingredient\",\n",
    "    \"add_info\": \"additional information not in the other json attributes\",\n",
    "}]\n",
    "json_output_string = json.dumps(json_output)\n",
    "\n",
    "prompt_template= \"You are a multilingual food ingredient parser agent. For each ingredient in the list that i will provide, give the following details in a RFC8259 compliant JSON response format. Do not include any explanations and only provide the json response without deviation.: \"+json_output_string + \"If there are multiple ingredients, put them in their respective json entities within an array and respective keyword filled.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mimic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
